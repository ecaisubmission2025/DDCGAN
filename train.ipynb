{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.nn.functional import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Dimensions of embeddings\n",
    "embedding_dim = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"data/final_data_cleaned.parquet\", engine='pyarrow')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df, random_seed=42):\n",
    "    # For reproducibility\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    df_train = pd.DataFrame(columns=df.columns)\n",
    "    df_test = pd.DataFrame(columns=df.columns)\n",
    "    \n",
    "    disease_counts = df.groupby('disease').size()\n",
    "    \n",
    "    processed_diseases = set()\n",
    "    \n",
    "    # For each possible number of prescriptions (1 to 10)\n",
    "    for n_prescriptions in range(1, 11):\n",
    "        # Get diseases with exactly n prescriptions\n",
    "        diseases_with_n = disease_counts[disease_counts == n_prescriptions].index.tolist()\n",
    "        \n",
    "        if len(diseases_with_n) > 0:\n",
    "            # Calculate number of diseases to move to test set (10%, minimum 1)\n",
    "            n_to_test = max(1, int(np.ceil(len(diseases_with_n) * 0.1)))\n",
    "            \n",
    "            # Randomly select diseases for test set\n",
    "            test_diseases = np.random.choice(diseases_with_n, \n",
    "                                          size=n_to_test, \n",
    "                                          replace=False)\n",
    "            \n",
    "            # Add to test set\n",
    "            test_mask = df['disease'].isin(test_diseases)\n",
    "            df_test = pd.concat([df_test, df[test_mask]])\n",
    "            \n",
    "            # Add remaining to train set\n",
    "            train_mask = df['disease'].isin(diseases_with_n) & ~df['disease'].isin(test_diseases)\n",
    "            df_train = pd.concat([df_train, df[train_mask]])\n",
    "            \n",
    "            # Add to processed diseases\n",
    "            processed_diseases.update(diseases_with_n)\n",
    "    \n",
    "    assert len(processed_diseases) == len(disease_counts), \"Not all diseases were processed\"\n",
    "    assert len(df) == len(df_train) + len(df_test), \"Row counts don't match\"\n",
    "    \n",
    "    return df_train.reset_index(drop=True), df_test.reset_index(drop=True)\n",
    "\n",
    "df, df_test_filtered = split_dataframe(df, random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle df_train and df_test\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_test_filtered = df_test_filtered.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 1024),\n",
    "            nn.LayerNorm(1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(512, embedding_dim),\n",
    "            nn.LayerNorm(embedding_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, disease_embedding):\n",
    "        return self.model(disease_embedding)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disease_encoder = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.drug_encoder = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, disease_embedding, drug_embedding):\n",
    "        disease_features = self.disease_encoder(disease_embedding)\n",
    "        drug_features = self.drug_encoder(drug_embedding)\n",
    "        combined = torch.cat([disease_features, drug_features], dim=1)\n",
    "        return self.classifier(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrugGAN:\n",
    "    def __init__(self, df, df_test_filtered, batch_size=32, lr_g=0.0001, lr_d=0.0001):\n",
    "        self.df = df\n",
    "        self.df_test_filtered = df_test_filtered\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        \n",
    "        # Convert embeddings to tensors and move to device\n",
    "        self.disease_embeddings = torch.tensor(\n",
    "            np.vstack(df[\"disease_embedding\"].values), \n",
    "            dtype=torch.float32\n",
    "        ).to(device)\n",
    "        self.drug_embeddings = torch.tensor(\n",
    "            np.vstack(df[\"drug_embedding\"].values), \n",
    "            dtype=torch.float32\n",
    "        ).to(device)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        self.disease_embeddings = nn.functional.normalize(self.disease_embeddings, dim=1)\n",
    "        self.drug_embeddings = nn.functional.normalize(self.drug_embeddings, dim=1)\n",
    "\n",
    "        # Convert and normalize test embeddings\n",
    "        self.disease_embeddings_test = torch.tensor(\n",
    "            np.vstack(df_test_filtered[\"disease_embedding\"].values), \n",
    "            dtype=torch.float32\n",
    "        ).to(device)\n",
    "        self.drug_embeddings_test = torch.tensor(\n",
    "            np.vstack(df_test_filtered[\"drug_embedding\"].values), \n",
    "            dtype=torch.float32\n",
    "        ).to(device)\n",
    "        self.disease_embeddings_test = nn.functional.normalize(self.disease_embeddings_test, dim=1)\n",
    "        self.drug_embeddings_test = nn.functional.normalize(self.drug_embeddings_test, dim=1)\n",
    "        \n",
    "        # Create mappings\n",
    "        self.disease_to_drugs = self._create_disease_drug_mapping(self.df)\n",
    "        self.disease_to_drugs_test = self._create_disease_drug_mapping(self.df_test_filtered)\n",
    "\n",
    "        self.disease_drug_map_test = {}\n",
    "\n",
    "        for disease in self.df_test_filtered['disease'].unique():\n",
    "            disease_drugs = self.df_test_filtered[self.df_test_filtered['disease'] == disease]['drug'].unique()\n",
    "            self.disease_drug_map_test[disease] = set(disease_drugs)\n",
    "        \n",
    "        # Initialize models\n",
    "        self.generator = Generator().to(device)\n",
    "        self.discriminator = Discriminator().to(device)\n",
    "        \n",
    "        # Initialize optimizers\n",
    "        self.optimizer_G = optim.Adam(\n",
    "            self.generator.parameters(), \n",
    "            lr=lr_g,\n",
    "            betas=(0.5, 0.999)\n",
    "        )\n",
    "        self.optimizer_D = optim.Adam(\n",
    "            self.discriminator.parameters(), \n",
    "            lr=lr_d,\n",
    "            betas=(0.5, 0.999)\n",
    "        )\n",
    "        \n",
    "        # Use different loss functions\n",
    "        self.adversarial_loss = nn.BCELoss()\n",
    "\n",
    "        # Add variables for tracking best similarity\n",
    "        self.best_similarity = -float('inf')\n",
    "        self.best_generator_state = None\n",
    "        self.stagnant_epochs = 0\n",
    "    \n",
    "    def _create_disease_drug_mapping(self, df):\n",
    "        \"\"\"Create a mapping of diseases to their valid drug embeddings\"\"\"\n",
    "        disease_drug_map = {}\n",
    "        for disease in df['disease'].unique():\n",
    "            disease_drugs = df[df['disease'] == disease]['drug_embedding'].values\n",
    "            embeddings = np.vstack(disease_drugs)\n",
    "            # Normalize embeddings\n",
    "            embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "            disease_drug_map[disease] = torch.tensor(\n",
    "                embeddings,\n",
    "                dtype=torch.float32\n",
    "            ).to(self.device)\n",
    "        return disease_drug_map\n",
    "    \n",
    "    def triplet_loss(self, anchor, positive, negatives, margin=0.3):\n",
    "        \"\"\"\n",
    "        Compute triplet loss with cosine similarity for top-k hard negatives.\n",
    "        \n",
    "        This function encourages the model to bring the anchor (generated drug embedding)\n",
    "        closer to the positive (valid drug for the disease) and farther from the \n",
    "        negatives (invalid or less relevant drugs), with a specified margin.\n",
    "        \n",
    "        - anchor: embedding of the generated drug\n",
    "        - positive: embedding of a valid drug for the target disease\n",
    "        - negatives: embeddings of top-k hard negative drugs\n",
    "        - margin: desired minimum gap between positive and negative distances\n",
    "        \n",
    "        The loss is computed as:\n",
    "            loss = mean(max(0, (1 - sim(anchor, positive)) - (1 - sim(anchor, negative)) + margin))\n",
    "                = mean(max(0, sim(anchor, negative) - sim(anchor, positive) + margin))\n",
    "        \n",
    "        This encourages the similarity to the positive to be greater than the similarity \n",
    "        to any negative by at least the given margin.\n",
    "        \"\"\"\n",
    "        # Normalize embeddings\n",
    "        anchor = nn.functional.normalize(anchor, dim=1)\n",
    "        positive = nn.functional.normalize(positive, dim=1)\n",
    "        negatives = nn.functional.normalize(negatives, dim=1)\n",
    "        \n",
    "        # Compute similarities\n",
    "        positive_sim = torch.cosine_similarity(anchor, positive)\n",
    "        negative_sims = torch.cosine_similarity(anchor, negatives)  # Multiple negatives\n",
    "        \n",
    "        # Convert similarities to distances (1 - similarity)\n",
    "        positive_dist = 1 - positive_sim\n",
    "        negative_dists = 1 - negative_sims  # Distance for all negatives\n",
    "        \n",
    "        # Compute loss for each negative\n",
    "        losses = torch.clamp(positive_dist - negative_dists + margin, min=0)\n",
    "        \n",
    "        # Average over all top-k negatives\n",
    "        return losses.mean()\n",
    "    \n",
    "    def compute_similarity_reward(self, generated_embedding, disease_name, test=False):\n",
    "        \"\"\"\n",
    "        Compute contrastive similarity reward using cosine similarity.\n",
    "        \n",
    "        This function evaluates how well the generated drug embedding aligns with \n",
    "        valid drugs for a given disease (positives) while penalizing similarity \n",
    "        to unrelated drugs (negatives).\n",
    "        \n",
    "        - generated_embedding: the embedding of the generated drug\n",
    "        - disease_name: disease used to determine positive/negative sets\n",
    "        - test: flag to indicate if test or training data should be used\n",
    "        \n",
    "        The reward is computed as:\n",
    "            reward = mean(similarity with valid drugs) - 0.75 * mean(similarity with invalid drugs)\n",
    "        \n",
    "        A higher reward indicates that the model generates embeddings closer to the \n",
    "        correct drug space while being dissimilar to irrelevant ones.\n",
    "        \"\"\"\n",
    "\n",
    "        valid_embeddings = (self.disease_to_drugs_test if test else self.disease_to_drugs)[disease_name]\n",
    "        \n",
    "        # Get positive samples (valid drugs for this disease)\n",
    "        positive_similarities = torch.cosine_similarity(\n",
    "            generated_embedding.unsqueeze(0),\n",
    "            valid_embeddings\n",
    "        )\n",
    "        \n",
    "        # Get negative samples (random drugs not valid for this disease)\n",
    "        all_drugs = self.drug_embeddings\n",
    "        # Create indices tensor\n",
    "        disease_indices = torch.tensor([\n",
    "            i for i, d in enumerate(self.df['disease']) if d == disease_name\n",
    "        ], dtype=torch.long, device=self.device)\n",
    "        \n",
    "        # Create negative mask\n",
    "        negative_mask = torch.ones(len(all_drugs), dtype=torch.bool, device=self.device)\n",
    "        negative_mask[disease_indices] = False\n",
    "        negative_embeddings = all_drugs[negative_mask]\n",
    "        \n",
    "        # Handle case where there might be no negative samples\n",
    "        if len(negative_embeddings) == 0:\n",
    "            return positive_similarities.mean(), positive_similarities.mean()\n",
    "        \n",
    "        negative_similarities = torch.cosine_similarity(\n",
    "            generated_embedding.unsqueeze(0),\n",
    "            negative_embeddings\n",
    "        )\n",
    "        \n",
    "        # Compute contrastive reward\n",
    "        positive_reward = positive_similarities.mean()\n",
    "        negative_penalty = negative_similarities.mean()\n",
    "        \n",
    "        # Reward higher similarity to valid drugs and lower similarity to invalid ones\n",
    "        reward = positive_reward - 0.75 * negative_penalty\n",
    "        \n",
    "        return reward, positive_reward\n",
    "    \n",
    "    def find_most_similar_drugs(self, disease_embedding, combined_df, device, top_n=5):\n",
    "        # Generate the disease embedding\n",
    "        disease_embedding = torch.tensor(disease_embedding, dtype=torch.float32).to(device)\n",
    "        disease_embedding = disease_embedding.unsqueeze(0)\n",
    "        \n",
    "        # Generate drug embedding\n",
    "        generated_drug_embedding = self.generate_drug(disease_embedding).cpu().numpy()\n",
    "        \n",
    "        # Normalize all drug embeddings\n",
    "        drug_embeddings = np.vstack(combined_df[\"drug_embedding\"].values)\n",
    "        drug_embeddings = normalize(torch.tensor(drug_embeddings), dim=1).numpy()\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarities = cosine_similarity(generated_drug_embedding, drug_embeddings)\n",
    "        \n",
    "        # Get indices sorted by similarity\n",
    "        sorted_indices = np.argsort(similarities[0])[::-1]\n",
    "        \n",
    "        # Retrieve the most similar drugs and ensure uniqueness\n",
    "        unique_drugs = []\n",
    "        for idx in sorted_indices:\n",
    "            drug = combined_df.iloc[idx][\"drug\"]\n",
    "            if drug not in unique_drugs:\n",
    "                unique_drugs.append(drug)\n",
    "            if len(unique_drugs) >= top_n:\n",
    "                break\n",
    "        \n",
    "        return unique_drugs\n",
    "    \n",
    "    def compute_gradient_penalty(self, real_samples, fake_samples, disease_batch):\n",
    "        alpha = torch.rand((real_samples.size(0), 1), device=self.device)\n",
    "        alpha = alpha.expand(real_samples.size())\n",
    "        interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "        d_interpolates = self.discriminator(disease_batch, interpolates)\n",
    "        fake = torch.ones(d_interpolates.size(), device=self.device)\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=d_interpolates,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=fake,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]\n",
    "        gradients = gradients.view(gradients.size(0), -1)\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "        return gradient_penalty\n",
    "    \n",
    "    def evaluate_multiple_accuracy(self, df_eval, df_combined, disease_drug_map, device, top_n=5):\n",
    "        correct_predictions = 0\n",
    "        \n",
    "        for _, row in df_eval.iterrows():\n",
    "            disease_name, disease_embedding = row['disease'], row['disease_embedding']\n",
    "            \n",
    "            predicted_drugs = self.find_most_similar_drugs(\n",
    "                disease_embedding,\n",
    "                df_combined,\n",
    "                device,\n",
    "                top_n=top_n\n",
    "            )\n",
    "            \n",
    "            # Check if any of the predicted drugs are in the set of valid drugs for this disease\n",
    "            if any(predicted_drug in disease_drug_map[disease_name] for predicted_drug in predicted_drugs):\n",
    "                correct_predictions += 1\n",
    "                \n",
    "        accuracy = correct_predictions / len(df_eval)\n",
    "        return accuracy\n",
    "    \n",
    "    def train_step(self, disease_batch, drug_batch, disease_names):\n",
    "        batch_size = disease_batch.size(0)\n",
    "        real_labels = torch.ones((batch_size, 1), device=self.device)\n",
    "        fake_labels = torch.zeros((batch_size, 1), device=self.device)\n",
    "        \n",
    "        # Train Discriminator\n",
    "        self.optimizer_D.zero_grad()\n",
    "        \n",
    "        # Real samples\n",
    "        real_pred = self.discriminator(disease_batch, drug_batch)\n",
    "        d_real_loss = self.adversarial_loss(real_pred, real_labels)\n",
    "        \n",
    "        # Generate fake samples\n",
    "        generated_drugs = self.generator(disease_batch)\n",
    "        generated_drugs = nn.functional.normalize(generated_drugs, dim=1)\n",
    "        fake_pred = self.discriminator(disease_batch, generated_drugs.detach())\n",
    "        d_fake_loss = self.adversarial_loss(fake_pred, fake_labels)\n",
    "        \n",
    "        # Compute gradient penalty\n",
    "        gradient_penalty = self.compute_gradient_penalty(drug_batch, generated_drugs, disease_batch)\n",
    "        lambda_gp = 10  # Coefficient for the gradient penalty term\n",
    "        \n",
    "        # Total discriminator loss with gradient penalty\n",
    "        d_loss = d_real_loss + d_fake_loss + lambda_gp * gradient_penalty\n",
    "        d_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), max_norm=1.0)\n",
    "        self.optimizer_D.step()\n",
    "        \n",
    "        # Train Generator\n",
    "        self.optimizer_G.zero_grad()\n",
    "        \n",
    "        generated_drugs = self.generator(disease_batch)\n",
    "        generated_drugs = nn.functional.normalize(generated_drugs, dim=1)\n",
    "        \n",
    "        # Adversarial loss\n",
    "        fake_pred = self.discriminator(disease_batch, generated_drugs)\n",
    "        g_loss = self.adversarial_loss(fake_pred, real_labels)\n",
    "        \n",
    "        # Initialize total triplet loss\n",
    "        triplet_losses = []\n",
    "        similarity_rewards = torch.zeros(batch_size, device=self.device)\n",
    "        similarity_scores = torch.zeros(batch_size, device=self.device)\n",
    "        \n",
    "        for i, (gen_drug, disease_name) in enumerate(zip(generated_drugs, disease_names)):\n",
    "            # Get positive samples (valid drugs for this disease)\n",
    "            valid_drugs = self.disease_to_drugs[disease_name]\n",
    "            \n",
    "            # Get negative samples\n",
    "            disease_indices = torch.tensor([\n",
    "                i for i, d in enumerate(self.df['disease']) if d == disease_name\n",
    "            ], dtype=torch.long, device=self.device)\n",
    "            negative_mask = torch.ones(len(self.drug_embeddings), dtype=torch.bool, device=self.device)\n",
    "            negative_mask[disease_indices] = False\n",
    "            negative_drugs = self.drug_embeddings[negative_mask]\n",
    "            \n",
    "            # Sample hardest negative (most similar to generated)\n",
    "            with torch.no_grad():\n",
    "                neg_sims = torch.cosine_similarity(\n",
    "                    gen_drug.unsqueeze(0),\n",
    "                    negative_drugs\n",
    "                )\n",
    "                # Select top-k hardest negatives\n",
    "                top_k_neg_indices = torch.topk(neg_sims, k=5, largest=True)[1]\n",
    "                top_k_negatives = negative_drugs[top_k_neg_indices]\n",
    "            \n",
    "            # Sample random positive\n",
    "            pos_idx = torch.randint(0, len(valid_drugs), (1,))\n",
    "            positive = valid_drugs[pos_idx]\n",
    "            \n",
    "            # Compute triplet loss with averaged top-k negatives\n",
    "            triplet_losses.append(\n",
    "                self.triplet_loss(\n",
    "                    gen_drug.unsqueeze(0),\n",
    "                    positive,\n",
    "                    top_k_negatives,  # Use top-k negatives\n",
    "                    margin=0.3\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Compute regular similarity reward\n",
    "            reward, similarity = self.compute_similarity_reward(gen_drug, disease_name)\n",
    "            similarity_rewards[i] = reward\n",
    "            similarity_scores[i] = similarity\n",
    "        \n",
    "        triplet_loss = torch.stack(triplet_losses).mean()\n",
    "        similarity_loss = -torch.mean(similarity_rewards)\n",
    "        \n",
    "        # Combined loss with weighted components\n",
    "        g_total_loss = (\n",
    "            1.0 * g_loss +           # adversarial loss\n",
    "            2.0 * similarity_loss +  # contrastive loss\n",
    "            1.0 * triplet_loss       # triplet loss\n",
    "        )\n",
    "        \n",
    "        g_total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.generator.parameters(), max_norm=1.0)\n",
    "        self.optimizer_G.step()\n",
    "        \n",
    "        return {\n",
    "            'g_loss': g_total_loss.item(),\n",
    "            'd_loss': d_loss.item(),\n",
    "            'adversarial_loss': g_loss.item(),\n",
    "            'similarity_mean': similarity_scores.mean().item(),\n",
    "            'triplet_loss': triplet_loss.item()\n",
    "        }\n",
    "    \n",
    "    def train(self, num_epochs=500, patience=50):\n",
    "        from tqdm.auto import tqdm, trange\n",
    "        \n",
    "        disease_indices = torch.tensor(range(len(self.df)), dtype=torch.long)\n",
    "        dataset = TensorDataset(\n",
    "            self.disease_embeddings,\n",
    "            self.drug_embeddings,\n",
    "            disease_indices\n",
    "        )\n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        # Create progress bar for epochs\n",
    "        epoch_pbar = trange(num_epochs, desc='Training')\n",
    "        \n",
    "        for epoch in epoch_pbar:\n",
    "            g_losses = []\n",
    "            d_losses = []\n",
    "            \n",
    "            # Create progress bar for batches\n",
    "            batch_pbar = tqdm(\n",
    "                dataloader,\n",
    "                desc=f'Epoch {epoch}',\n",
    "                leave=False\n",
    "            )\n",
    "            \n",
    "            for disease_batch, drug_batch, indices in batch_pbar:\n",
    "                disease_names = [self.df['disease'].iloc[idx.item()] for idx in indices]\n",
    "                \n",
    "                metrics = self.train_step(\n",
    "                    disease_batch,\n",
    "                    drug_batch,\n",
    "                    disease_names\n",
    "                )\n",
    "                \n",
    "                g_losses.append(metrics['g_loss'])\n",
    "                d_losses.append(metrics['d_loss'])\n",
    "                \n",
    "                # Update batch progress bar\n",
    "                batch_pbar.set_postfix({\n",
    "                    'G_loss': f\"{metrics['g_loss']:.4f}\",\n",
    "                    'D_loss': f\"{metrics['d_loss']:.4f}\",\n",
    "                    'Adversarial_loss': f\"{metrics['adversarial_loss']:.4f}\",\n",
    "                    'Sim': f\"{metrics['similarity_mean']:.4f}\",\n",
    "                    'Triplet_loss': f\"{metrics['triplet_loss']:.4f}\"\n",
    "                })\n",
    "            \n",
    "            # Update learning rates\n",
    "            avg_g_loss = np.mean(g_losses)\n",
    "            avg_d_loss = np.mean(d_losses)\n",
    "            \n",
    "            # Compute test similarity\n",
    "            test_similarity = self.evaluate_multiple_accuracy(\n",
    "                self.df_test_filtered,\n",
    "                self.df_test_filtered,\n",
    "                self.disease_drug_map_test,\n",
    "                device,\n",
    "                top_n=5\n",
    "            )\n",
    "            \n",
    "            # Update epoch progress bar\n",
    "            epoch_pbar.set_postfix({\n",
    "                'G_loss': f\"{avg_g_loss:.4f}\",\n",
    "                'D_loss': f\"{avg_d_loss:.4f}\",\n",
    "                'Test_Sim': f\"{test_similarity:.4f}\",\n",
    "                'LR_G': f\"{self.optimizer_G.param_groups[0]['lr']:.6f}\",\n",
    "                'LR_D': f\"{self.optimizer_D.param_groups[0]['lr']:.6f}\"\n",
    "            })\n",
    "\n",
    "            # Early stopping check\n",
    "            if test_similarity > self.best_similarity:\n",
    "                print(f\"Updating the best model state at epoch {epoch}, model test similarity: {test_similarity:.4f}\")\n",
    "                self.best_similarity = test_similarity\n",
    "                self.best_generator_state = copy.deepcopy(self.generator.state_dict())\n",
    "                self.stagnant_epochs = 0\n",
    "            else:\n",
    "                self.stagnant_epochs += 1\n",
    "\n",
    "            # Early stopping condition\n",
    "            if self.stagnant_epochs >= patience:\n",
    "                print(f\"\\nStopping early at epoch {epoch} due to no improvement in test similarity.\")\n",
    "                if self.best_generator_state is not None:\n",
    "                    self.generator.load_state_dict(self.best_generator_state)\n",
    "                    restored_similarity = self.evaluate_multiple_accuracy(\n",
    "                        self.df_test_filtered,\n",
    "                        self.df_test_filtered,\n",
    "                        self.disease_drug_map_test,\n",
    "                        device,\n",
    "                        top_n=5\n",
    "                    )\n",
    "                    print(f\"Restored model test similarity: {restored_similarity:.4f} (Expected: {self.best_similarity:.4f})\")\n",
    "                break\n",
    "\n",
    "\n",
    "    def generate_drug(self, disease_embedding):\n",
    "        if not isinstance(disease_embedding, torch.Tensor):\n",
    "            disease_embedding = torch.tensor(\n",
    "                disease_embedding, \n",
    "                dtype=torch.float32\n",
    "            ).to(self.device)\n",
    "        \n",
    "        if len(disease_embedding.shape) == 1:\n",
    "            disease_embedding = disease_embedding.unsqueeze(0)\n",
    "        \n",
    "        # Normalize input\n",
    "        disease_embedding = nn.functional.normalize(disease_embedding, dim=1)\n",
    "        \n",
    "        self.generator.eval()\n",
    "        with torch.no_grad():\n",
    "            generated_drug = self.generator(disease_embedding)\n",
    "            generated_drug = nn.functional.normalize(generated_drug, dim=1)\n",
    "        self.generator.train()\n",
    "        \n",
    "        return generated_drug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DrugGAN(df, df_test_filtered, batch_size=32)\n",
    "model.train(num_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.generator.state_dict(), 'models/generator_63_17.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
